# -*- coding: utf-8 -*-
"""topic_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZ3nhtLh9yia81qXI1UIbMGEa29w6esS

# Topic clustering

Topic clustering is unsupervised task since we use unlabeled data. The aim of topic clustering is to categorize corpus of text into homogenized clusters (just like what we have in clustering tasks).

In this project, I use a sample of 2000 news texts as the data source. Then, I apply various text cleaning and preprocessing techniques to make the corpus ready for various clustering methods. Afterwards, I use kmeans and LDA to cluster the text and compare the results.
"""

# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# NLP
import nltk
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
import gensim
import gensim.corpora as corpora
from gensim.corpora import Dictionary
from gensim.models import TfidfModel, LdaModel,LdaMulticore, CoherenceModel
import spacy
from gensim.utils import simple_preprocess
from collections import Counter

# reading the data
df = pd.read_csv("https://github.com/sajadahmadia/NLP/raw/main/topic_clustering/raw_data.csv")
print(df.shape)
df.head()

"""As it appears, our dataset consists of  news articles from NYT(New York Times), Wired, and .... (different news agencies) including their keywords, categories, full text body, time of pulication, and ... . There is also an additional column (Unnamed: 0) that should be removed."""

# dropping the Unnamed: 0 column
df.drop(columns = {"Unnamed: 0"}, inplace = True)
df.columns

"""## Text preprocessing

At this step, I use different preprocessing techniques to prepare the corpus for using in topic clustering algorithms. I use the body column of the dataframe since it contains the largest amount of data in comparison to other columns, but using other columns or using multiple of them is also recommened for future studies.
"""

# loading the English language model provided by spaCy
nlp = spacy.load("en_core_web_sm")

"""**Text tokenization:**

using the above nlp function, I transform the text in the body column into a list of separated words. The nlp function not only tokenizes the words but also do a whole nlp process on them, ie, it retrieves some meta data for each word like Part of Speech tag for each word.
"""

df['text_nlp'] = df['body'].apply(lambda x: nlp(str(x)))

df[['body','text_nlp']].head()

"""As we can see, the text_nlp column now contains the tokenized version fo the body column. It also includes some other meta data like the part of speech tag of the word(ie, whether the word is a noun, verb, adverb, ....). You can see the universal part of speech tags [here](https://universaldependencies.org/u/pos/) . To show the meaning of Part of Speech(POS), I apply it on the first row of the body column

"""

first_row = nlp(df.iloc[0,-2])

spacy.displacy.render(first_row, style='ent', jupyter=True)

"""In the next step, I choose only some part of speech tags that are more meaningful(in my opinion) for clustering texts."""

def include_features(x):
    include_features = ['VERB', 'PROPN', 'NOUN', 'ADJ', 'PRON']
    text = ' '.join([ent.text for ent in x if ent.pos_ in include_features])
    return text

df['features'] = df['text_nlp'].apply(lambda x: include_features(x))

df['features']

"""As it looks, some words in each row is removed(those that were not part of the mentioned POS tags). As an example, I investigate the length of the first body before and after applying the POS tagging filter:

"""

print(f"the length of the first body before filtering based on POS tags {len(df.iloc[0,-3])}\nafter filtering the tags {len(df.iloc[0,-1])}")

"""### Removing stop words and retrieving the lemma form of words

* Stop words are set of commonly used words in a language. Examples of stop words in English are "a","an","the","your",... . We usually remove these words while doing nlp tasks to reach more important words

* lemma of a word is the root form of that word. for example, the lemma of "are", "is", and "was" is "be". In nlp, we usually use the lemmatized form a word to ignore the grammatical differences of the same root word.

In the next step, I create a function to first remove punctuation and lowercase words while returning a tokenized version of each text, then remove stop words and finally gets the lemmatized form of each word.
"""

# retreiving the English stop words and adding "and" as an additional word to it
stop_words = stopwords.words('english')
stop_words.extend(["and"])

# using nltk library to get the lemmatized version of words
lemmatizer = nltk.stem.WordNetLemmatizer()

def clean_text(text):
    tokens = simple_preprocess(text)
    # remove stopwords
    text = [token for token in tokens if not token in stop_words]
    # lemmatization
    text = [lemmatizer.lemmatize(word) for word in text]
    return text

df['features']

df['features'] = df['features'].apply(lambda text: clean_text(text))
df['features']

"""As an example, the first three words of the first body (without applying any preprocessing) is "bots console resellers snatching", while after applying the whole preprocessing process it became "bot console resellers snatching". The lemmatizer function removed the plural "s" from the "bots" word.

### TF-IDF(Term Frequency-Inverse Document Frequency)
TF-IDF shows the importance of a word in a document amoung a collection of documents (here, body column).
TF is calculated as:

* **TF(t,d)**= Total number of terms in document d / Number of times term t appears in document d

and IDF is calculated as:

* **IDF(t,D)**=log(
Number of documents containing term (t+1)/
Total number of documents in the corpus N
â€‹
 )

corpus is the collection of all texts, here the body column.

Finally, TF-IDF is the multiplication of these two scores, which shows the importance of a word(and not only the frequency of a word).

To get the TF-IDF score, first, I need to create a Gensim Dictionary from the preprocessed column.
"""

# Create a Gensim Dictionary from the documents
gensim_dict = Dictionary(df['features'])

# Convert the documents to a bag-of-words format using the dictionary
corpus = [gensim_dict.doc2bow(doc) for doc in df['features']]

# Convert the TF-IDF matrix to Gensim's sparse format
tfidf = TfidfModel(corpus, dictionary=gensim_dict)
corpus = tfidf[corpus]

"""To get an understanding of what happened after applying the tf-idf model, I just print the first values of the first 5 documents.Showing all the tf-idf values even for the first document(which has 3810 words) might not be useful:"""

# display the words and their TF-IDF values:
for i, doc_tfidf in zip(range(2),corpus):
    print(f"\nDocument {i + 1} TF-IDF values:")
    for word_id, tfidf_value in doc_tfidf[:10]:
        word = gensim_dict[word_id]
        print(f"{word}: {tfidf_value:.4f}")

"""## Topic Clustering
Finally, we come to the interesting part: topic clustering! I use LDA algorithm (Latent Dirichlet Allocation) to find tipic clusters. LDA uses Dirichlet distributions to model the mixture of topics in documents and the distribution of words in topics. Dirichlet distributions are a family of continuous probability distributions often used to model the distribution of probabilities over a fixed set of possible outcomes. For a detailed explanation, see this [link](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). But before getting into the interesting part, we should come up with an idea to address a frequently asked topic:

### A highly-frequent topic: Determining the number of clusters

Since clustering is an unsupervised technique, determining the number of clusters(especially in those algorithms that we can pre-determine the number of clusters) is an arbitrary choice that might affect our results hugely. As a result, I try different number of clusters and use a coherence metric to calculate topic coherence for each model.
"""

# Train the Gensim LDA model
num_topics = 5
gensim_lda = LdaModel(corpus, num_topics=num_topics, id2word=gensim_dict)

topics = []
score = []
models = []
for i in range(2,21,2):
   lda_model = LdaMulticore(corpus=corpus, id2word=gensim_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)
   cm = CoherenceModel(model=lda_model, texts = df['features'], corpus=corpus, dictionary=gensim_dict, coherence='c_v')
   topics.append(i)
   models.append(lda_model)
   score.append(cm.get_coherence())

"""Although I used the faster version of lda model function(LdaMulticore), it took almost 30 minutes to run. Since I used c_v as the coherene parameter, it returns a value between 0 and 1, the nearer to 1 the better. In the next step, I use the elbow rule to find the best number of clusters that stands out for the number of clusters(model complexity)-performance tradeoff."""

#plotting the c_v value per number of clusters
plt.bar(topics, score)
plt.xticks(topics, np.arange(2,21,2))
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Score')
plt.show()

"""As it seems, using 10 clusters might be good enough."""

selected_model = models[4]

# Get a single topic as a formatted string, a combination of coefficients * words per topic cluster
selected_model.print_topics()

"""finding topics in LDA is not as easy as what we see in KMeans or other clustering algorithms. LDA assigns a probability score for each document to be assigned to each topic cluser. For simplicity, I show the first 5 documents an their assigned cluster probabilities. For more information, use the gensim documentation [here](https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore)"""

# Get the topic distribution for each document
topic_documents = selected_model.get_document_topics(corpus)
[doc for doc in topic_documents[:5]]

"""gensim has a helper function for converting to and from numpy/scipy arrays. I convert the topics into a numpy array and then a dataframe to show them:"""

all_topics_csr = gensim.matutils.corpus2csc(topic_documents)
all_topics_numpy = all_topics_csr.T.toarray()
topic_probability = pd.DataFrame(all_topics_numpy)
topic_probability.head()

"""for the first document, the second cluster(cluster at inex = 1) has the highest probability(about 0.89). for the second document, again the second cluster has the highest probability. and ... . I get the max of each row and assign it as our cluster number. I also add a +1 to cluster numbers to make it more human friendly."""

# finding the max. value per row, retrieving its column name, adding +1 to it
clusters = topic_probability.idxmax(axis="columns")+1
# assigning the clusters to a new column
df['clusters'] = clusters
df['clusters'].head()

# checking the number of observations per cluster
df['clusters'].value_counts()

"""ooh, the second cluster encompasses almost more than 95% of all the observations! Let's see the highest frequency words for each topic. To do so, I should first group all 'features' per cluster to make a single list for each cluster. then, use the counter function of the collection library to easily count the number of words per cluster:"""

# stack list of words together
cluster_words = df.groupby('clusters')['features'].sum()

# count the frequency of each word in each cluster
cluster_word_counts = cluster_words.apply(Counter)

# get the top 10 words per cluster
top_words_per_cluster = cluster_word_counts.apply(lambda x: x.most_common(10))

# showing the result
for cluster, top_words in top_words_per_cluster.items():
    print(f"\nCluster {cluster} - Top {10} words:")
    for word, count in top_words:
        print(f"{word}: {count}")

"""Based on my understanding, and if I want to name the clusters based on their top words, I come up with these names:
* cluster 2: people work in newyork time!
* cluster 4: countries
* cluster 6: nan, which is awkward and needs more investigation.
* cluster 7: clothes

**No need to mention**, I didn't try to to clean up the unexpected results that might happen when doing topic clustering, because it is the reality of our job. Each investigation provides some ideas for further and better models, and we come up with a newer model, and again and again ....

# Conclusion

Topic clustering is a widespread task in NLP. Although it seems at the first glance, It requires further investigations and human checks. In this project, I tried to be fair and didn't manipulated the data to get just nice results; Meanwhile, I wanted to show the reality of the first results that we might get when doing an unsupervised nlp task.

Totally, I found only foure meaningful clusters and then named the clusters to a more human-friendly names.

# Future Actions
For future actions, I use the word lemmatizer from the spacy library instead of the nltk library and see the results, Since I see both "said" and "say" in the first cluster as high frequent words. The word lemmatizer from the nltk doesn't convert past tense verbs like "said" to their present tenses(like "say").

I also might try kmeans and other clustering methods and compare results.
"""

